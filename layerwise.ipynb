{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swin Transformer block\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_heads=1):\n",
    "        super(SwinTransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=in_channels, num_heads=num_heads)\n",
    "        self.linear1 = nn.Linear(in_channels, out_channels)\n",
    "        self.linear2 = nn.Linear(out_channels, out_channels)\n",
    "        self.norm1 = nn.LayerNorm(out_channels)\n",
    "        self.norm2 = nn.LayerNorm(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1, x.size(1)) \n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.linear1(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.size(0), x.size(2), int(x.size(1)**0.5), int(x.size(1)**0.5)) \n",
    "        return x\n",
    "\n",
    "# ConvNeXt block\n",
    "class ConvNeXtBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvNeXtBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "        self.linear = nn.Linear(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0, 2, 3, 1) \n",
    "        B, H, W, C = x.shape\n",
    "        x = x.contiguous().view(B * H * W, C) \n",
    "        x = self.norm(x)\n",
    "        x = x.view(B, H, W, C).permute(0, 3, 1, 2) \n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "# Dataset class\n",
    "class GarbageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, cls)\n",
    "            for img in os.listdir(class_dir):\n",
    "                if img.endswith(\".jpg\") or img.endswith(\".png\"):\n",
    "                    self.image_paths.append(os.path.join(class_dir, img))\n",
    "                    self.labels.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Model\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.swin_transformer_block1 = SwinTransformerBlock(in_channels=3, out_channels=64)\n",
    "        self.convnext_block1 = ConvNeXtBlock(in_channels=3, out_channels=64)\n",
    "        self.spatial_attention_mechanism = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1)\n",
    "        self.classifier = nn.Linear(224*224, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        swin_output = self.swin_transformer_block1(x)\n",
    "        convnext_output = self.convnext_block1(x)\n",
    "        combined_output = torch.cat((swin_output, convnext_output), dim=1)\n",
    "        attention_output = self.spatial_attention_mechanism(combined_output)\n",
    "        attention_output = attention_output.view(attention_output.size(0), -1)\n",
    "        output = self.classifier(attention_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "dataset = datasets.ImageFolder(root='garbage_classification', transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model Initialization\n",
    "model = FusionModel()\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [1/388], Loss: 2.6143, Accuracy: 0.00%\n",
      "Epoch [1/5], Batch [2/388], Loss: 15.7057, Accuracy: 25.00%\n",
      "Epoch [1/5], Batch [3/388], Loss: 30.9916, Accuracy: 9.38%\n",
      "Epoch [1/5], Batch [4/388], Loss: 36.1518, Accuracy: 3.12%\n",
      "Epoch [1/5], Batch [5/388], Loss: 15.6798, Accuracy: 15.62%\n",
      "Epoch [1/5], Batch [6/388], Loss: 9.7320, Accuracy: 12.50%\n",
      "Epoch [1/5], Batch [7/388], Loss: 12.3907, Accuracy: 15.62%\n",
      "Epoch [1/5], Batch [8/388], Loss: 8.9203, Accuracy: 18.75%\n",
      "Epoch [1/5], Batch [9/388], Loss: 8.3321, Accuracy: 9.38%\n",
      "Epoch [1/5], Batch [10/388], Loss: 12.0785, Accuracy: 15.62%\n",
      "Epoch [1/5], Batch [11/388], Loss: 7.3263, Accuracy: 12.50%\n",
      "Epoch [1/5], Batch [12/388], Loss: 5.2710, Accuracy: 21.88%\n",
      "Epoch [1/5], Batch [13/388], Loss: 8.4153, Accuracy: 12.50%\n",
      "Epoch [1/5], Batch [14/388], Loss: 8.3306, Accuracy: 12.50%\n",
      "Epoch [1/5], Batch [15/388], Loss: 7.5753, Accuracy: 28.12%\n",
      "Epoch [1/5], Batch [16/388], Loss: 6.7087, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [17/388], Loss: 7.6724, Accuracy: 21.88%\n",
      "Epoch [1/5], Batch [18/388], Loss: 4.1880, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [19/388], Loss: 5.3657, Accuracy: 21.88%\n",
      "Epoch [1/5], Batch [20/388], Loss: 4.3958, Accuracy: 25.00%\n",
      "Epoch [1/5], Batch [21/388], Loss: 5.5093, Accuracy: 25.00%\n",
      "Epoch [1/5], Batch [22/388], Loss: 7.3826, Accuracy: 15.62%\n",
      "Epoch [1/5], Batch [23/388], Loss: 6.7963, Accuracy: 12.50%\n",
      "Epoch [1/5], Batch [24/388], Loss: 4.9151, Accuracy: 12.50%\n",
      "Epoch [1/5], Batch [25/388], Loss: 3.9715, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [26/388], Loss: 2.8211, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [27/388], Loss: 3.5864, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [28/388], Loss: 5.9717, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [29/388], Loss: 4.0474, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [30/388], Loss: 3.0880, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [31/388], Loss: 3.0963, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [32/388], Loss: 3.4498, Accuracy: 21.88%\n",
      "Epoch [1/5], Batch [33/388], Loss: 2.9407, Accuracy: 18.75%\n",
      "Epoch [1/5], Batch [34/388], Loss: 3.7512, Accuracy: 18.75%\n",
      "Epoch [1/5], Batch [35/388], Loss: 3.5303, Accuracy: 15.62%\n",
      "Epoch [1/5], Batch [36/388], Loss: 2.7063, Accuracy: 18.75%\n",
      "Epoch [1/5], Batch [37/388], Loss: 2.2216, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [38/388], Loss: 2.5629, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [39/388], Loss: 2.9478, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [40/388], Loss: 2.9355, Accuracy: 28.12%\n",
      "Epoch [1/5], Batch [41/388], Loss: 3.2947, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [42/388], Loss: 2.3532, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [43/388], Loss: 2.3988, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [44/388], Loss: 2.2531, Accuracy: 28.12%\n",
      "Epoch [1/5], Batch [45/388], Loss: 2.2004, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [46/388], Loss: 1.7169, Accuracy: 62.50%\n",
      "Epoch [1/5], Batch [47/388], Loss: 1.9400, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [48/388], Loss: 2.6790, Accuracy: 25.00%\n",
      "Epoch [1/5], Batch [49/388], Loss: 2.7395, Accuracy: 25.00%\n",
      "Epoch [1/5], Batch [50/388], Loss: 2.8497, Accuracy: 15.62%\n",
      "Epoch [1/5], Batch [51/388], Loss: 1.6053, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [52/388], Loss: 2.2350, Accuracy: 28.12%\n",
      "Epoch [1/5], Batch [53/388], Loss: 2.1119, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [54/388], Loss: 1.7638, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [55/388], Loss: 2.3762, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [56/388], Loss: 2.1969, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [57/388], Loss: 2.5406, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [58/388], Loss: 1.8686, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [59/388], Loss: 2.4521, Accuracy: 25.00%\n",
      "Epoch [1/5], Batch [60/388], Loss: 2.1061, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [61/388], Loss: 1.9795, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [62/388], Loss: 1.7686, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [63/388], Loss: 2.0780, Accuracy: 28.12%\n",
      "Epoch [1/5], Batch [64/388], Loss: 2.0118, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [65/388], Loss: 1.9913, Accuracy: 25.00%\n",
      "Epoch [1/5], Batch [66/388], Loss: 1.7907, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [67/388], Loss: 1.8464, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [68/388], Loss: 1.6765, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [69/388], Loss: 2.1341, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [70/388], Loss: 2.0954, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [71/388], Loss: 1.9379, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [72/388], Loss: 1.8745, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [73/388], Loss: 1.9147, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [74/388], Loss: 2.1813, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [75/388], Loss: 1.8535, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [76/388], Loss: 1.8721, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [77/388], Loss: 2.0845, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [78/388], Loss: 1.8504, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [79/388], Loss: 1.9063, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [80/388], Loss: 1.7162, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [81/388], Loss: 1.8604, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [82/388], Loss: 1.9590, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [83/388], Loss: 1.7460, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [84/388], Loss: 1.4630, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [85/388], Loss: 1.6678, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [86/388], Loss: 2.2299, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [87/388], Loss: 1.8951, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [88/388], Loss: 1.9837, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [89/388], Loss: 1.9197, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [90/388], Loss: 2.2114, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [91/388], Loss: 1.6218, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [92/388], Loss: 1.4248, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [93/388], Loss: 1.5259, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [94/388], Loss: 1.4121, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [95/388], Loss: 2.1257, Accuracy: 25.00%\n",
      "Epoch [1/5], Batch [96/388], Loss: 1.2915, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [97/388], Loss: 1.6264, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [98/388], Loss: 1.6608, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [99/388], Loss: 1.6403, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [100/388], Loss: 1.6775, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [101/388], Loss: 1.8260, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [102/388], Loss: 1.8379, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [103/388], Loss: 1.9003, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [104/388], Loss: 1.9914, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [105/388], Loss: 1.8306, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [106/388], Loss: 2.2253, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [107/388], Loss: 1.2205, Accuracy: 68.75%\n",
      "Epoch [1/5], Batch [108/388], Loss: 2.0088, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [109/388], Loss: 2.0388, Accuracy: 28.12%\n",
      "Epoch [1/5], Batch [110/388], Loss: 1.4870, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [111/388], Loss: 1.5031, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [112/388], Loss: 1.7963, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [113/388], Loss: 1.5517, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [114/388], Loss: 2.2590, Accuracy: 25.00%\n",
      "Epoch [1/5], Batch [115/388], Loss: 1.4659, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [116/388], Loss: 2.0651, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [117/388], Loss: 1.7385, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [118/388], Loss: 1.7542, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [119/388], Loss: 1.8730, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [120/388], Loss: 1.5991, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [121/388], Loss: 1.5726, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [122/388], Loss: 1.8196, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [123/388], Loss: 1.9591, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [124/388], Loss: 1.8715, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [125/388], Loss: 1.4863, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [126/388], Loss: 1.8002, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [127/388], Loss: 1.7079, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [128/388], Loss: 1.6639, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [129/388], Loss: 1.6954, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [130/388], Loss: 1.8515, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [131/388], Loss: 1.8442, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [132/388], Loss: 1.4151, Accuracy: 68.75%\n",
      "Epoch [1/5], Batch [133/388], Loss: 1.8157, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [134/388], Loss: 1.2747, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [135/388], Loss: 1.5807, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [136/388], Loss: 1.5614, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [137/388], Loss: 1.8654, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [138/388], Loss: 1.6066, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [139/388], Loss: 1.6332, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [140/388], Loss: 1.9386, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [141/388], Loss: 1.4612, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [142/388], Loss: 1.7909, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [143/388], Loss: 1.9522, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [144/388], Loss: 2.0998, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [145/388], Loss: 1.8813, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [146/388], Loss: 2.0573, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [147/388], Loss: 1.9281, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [148/388], Loss: 1.4827, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [149/388], Loss: 1.7370, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [150/388], Loss: 1.9664, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [151/388], Loss: 1.5837, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [152/388], Loss: 1.4864, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [153/388], Loss: 1.8163, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [154/388], Loss: 1.9848, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [155/388], Loss: 1.9467, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [156/388], Loss: 1.5755, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [157/388], Loss: 1.6234, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [158/388], Loss: 2.0039, Accuracy: 28.12%\n",
      "Epoch [1/5], Batch [159/388], Loss: 1.8952, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [160/388], Loss: 1.8989, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [161/388], Loss: 1.8368, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [162/388], Loss: 2.1364, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [163/388], Loss: 1.6094, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [164/388], Loss: 1.6816, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [165/388], Loss: 1.6282, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [166/388], Loss: 1.3827, Accuracy: 62.50%\n",
      "Epoch [1/5], Batch [167/388], Loss: 1.6213, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [168/388], Loss: 2.1408, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [169/388], Loss: 1.6323, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [170/388], Loss: 1.4210, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [171/388], Loss: 1.4739, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [172/388], Loss: 1.8435, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [173/388], Loss: 1.7961, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [174/388], Loss: 1.7379, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [175/388], Loss: 1.7440, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [176/388], Loss: 1.6536, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [177/388], Loss: 1.2897, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [178/388], Loss: 1.6100, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [179/388], Loss: 1.7181, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [180/388], Loss: 1.6322, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [181/388], Loss: 1.6527, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [182/388], Loss: 2.0066, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [183/388], Loss: 2.1819, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [184/388], Loss: 1.5935, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [185/388], Loss: 1.6280, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [186/388], Loss: 1.8241, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [187/388], Loss: 2.0979, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [188/388], Loss: 1.6431, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [189/388], Loss: 1.2931, Accuracy: 62.50%\n",
      "Epoch [1/5], Batch [190/388], Loss: 1.5984, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [191/388], Loss: 2.0283, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [192/388], Loss: 1.3811, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [193/388], Loss: 1.4757, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [194/388], Loss: 1.2249, Accuracy: 62.50%\n",
      "Epoch [1/5], Batch [195/388], Loss: 1.7179, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [196/388], Loss: 1.4379, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [197/388], Loss: 1.6734, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [198/388], Loss: 1.3615, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [199/388], Loss: 1.5515, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [200/388], Loss: 1.4774, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [201/388], Loss: 1.9523, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [202/388], Loss: 1.8099, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [203/388], Loss: 1.8575, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [204/388], Loss: 1.2319, Accuracy: 62.50%\n",
      "Epoch [1/5], Batch [205/388], Loss: 1.4895, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [206/388], Loss: 1.7746, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [207/388], Loss: 1.3869, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [208/388], Loss: 1.5645, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [209/388], Loss: 1.5942, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [210/388], Loss: 1.5077, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [211/388], Loss: 1.5972, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [212/388], Loss: 2.3886, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [213/388], Loss: 1.2746, Accuracy: 65.62%\n",
      "Epoch [1/5], Batch [214/388], Loss: 1.5216, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [215/388], Loss: 2.1556, Accuracy: 28.12%\n",
      "Epoch [1/5], Batch [216/388], Loss: 1.6610, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [217/388], Loss: 1.7411, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [218/388], Loss: 1.6757, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [219/388], Loss: 2.1178, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [220/388], Loss: 1.5001, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [221/388], Loss: 1.8948, Accuracy: 28.12%\n",
      "Epoch [1/5], Batch [222/388], Loss: 1.6183, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [223/388], Loss: 1.6275, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [224/388], Loss: 1.7496, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [225/388], Loss: 1.2502, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [226/388], Loss: 2.1953, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [227/388], Loss: 1.6033, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [228/388], Loss: 1.4780, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [229/388], Loss: 1.2745, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [230/388], Loss: 1.7222, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [231/388], Loss: 1.9659, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [232/388], Loss: 2.0165, Accuracy: 28.12%\n",
      "Epoch [1/5], Batch [233/388], Loss: 1.7688, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [234/388], Loss: 1.5965, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [235/388], Loss: 1.7287, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [236/388], Loss: 1.6655, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [237/388], Loss: 1.9086, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [238/388], Loss: 1.7550, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [239/388], Loss: 1.8643, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [240/388], Loss: 2.0376, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [241/388], Loss: 1.7899, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [242/388], Loss: 1.7940, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [243/388], Loss: 1.2492, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [244/388], Loss: 1.9262, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [245/388], Loss: 1.9425, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [246/388], Loss: 1.7145, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [247/388], Loss: 1.4444, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [248/388], Loss: 1.9402, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [249/388], Loss: 1.4941, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [250/388], Loss: 1.9123, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [251/388], Loss: 1.4510, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [252/388], Loss: 1.6311, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [253/388], Loss: 1.7016, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [254/388], Loss: 1.5711, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [255/388], Loss: 1.3572, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [256/388], Loss: 1.8992, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [257/388], Loss: 1.3744, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [258/388], Loss: 1.6957, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [259/388], Loss: 2.1175, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [260/388], Loss: 1.7620, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [261/388], Loss: 1.6450, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [262/388], Loss: 1.9730, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [263/388], Loss: 1.4500, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [264/388], Loss: 1.8513, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [265/388], Loss: 2.0021, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [266/388], Loss: 1.3290, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [267/388], Loss: 1.1280, Accuracy: 71.88%\n",
      "Epoch [1/5], Batch [268/388], Loss: 1.3548, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [269/388], Loss: 1.6467, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [270/388], Loss: 1.9221, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [271/388], Loss: 1.6241, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [272/388], Loss: 1.9583, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [273/388], Loss: 1.8337, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [274/388], Loss: 1.3223, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [275/388], Loss: 1.7932, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [276/388], Loss: 1.8211, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [277/388], Loss: 1.6539, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [278/388], Loss: 1.4784, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [279/388], Loss: 1.4885, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [280/388], Loss: 1.3829, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [281/388], Loss: 1.5540, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [282/388], Loss: 1.8540, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [283/388], Loss: 1.3832, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [284/388], Loss: 1.7267, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [285/388], Loss: 1.5375, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [286/388], Loss: 1.4931, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [287/388], Loss: 1.6943, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [288/388], Loss: 1.8948, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [289/388], Loss: 1.8793, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [290/388], Loss: 1.5973, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [291/388], Loss: 1.3013, Accuracy: 68.75%\n",
      "Epoch [1/5], Batch [292/388], Loss: 2.3665, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [293/388], Loss: 1.3886, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [294/388], Loss: 1.8043, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [295/388], Loss: 1.3695, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [296/388], Loss: 1.5758, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [297/388], Loss: 1.5338, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [298/388], Loss: 1.5870, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [299/388], Loss: 1.5133, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [300/388], Loss: 1.9258, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [301/388], Loss: 1.7764, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [302/388], Loss: 1.6505, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [303/388], Loss: 1.7265, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [304/388], Loss: 1.4744, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [305/388], Loss: 1.6521, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [306/388], Loss: 1.7035, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [307/388], Loss: 1.5327, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [308/388], Loss: 1.9173, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [309/388], Loss: 1.4515, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [310/388], Loss: 1.9272, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [311/388], Loss: 2.0602, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [312/388], Loss: 1.4462, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [313/388], Loss: 1.7132, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [314/388], Loss: 1.3336, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [315/388], Loss: 1.6136, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [316/388], Loss: 1.5510, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [317/388], Loss: 1.8661, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [318/388], Loss: 1.6430, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [319/388], Loss: 1.5898, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [320/388], Loss: 1.8871, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [321/388], Loss: 1.5390, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [322/388], Loss: 1.6597, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [323/388], Loss: 1.6860, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [324/388], Loss: 1.5730, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [325/388], Loss: 1.3741, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [326/388], Loss: 1.5863, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [327/388], Loss: 1.6544, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [328/388], Loss: 1.8935, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [329/388], Loss: 1.3357, Accuracy: 65.62%\n",
      "Epoch [1/5], Batch [330/388], Loss: 1.3894, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [331/388], Loss: 1.6539, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [332/388], Loss: 2.1650, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [333/388], Loss: 1.5003, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [334/388], Loss: 1.7589, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [335/388], Loss: 2.1112, Accuracy: 28.12%\n",
      "Epoch [1/5], Batch [336/388], Loss: 0.7620, Accuracy: 75.00%\n",
      "Epoch [1/5], Batch [337/388], Loss: 1.4281, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [338/388], Loss: 1.8321, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [339/388], Loss: 1.9854, Accuracy: 31.25%\n",
      "Epoch [1/5], Batch [340/388], Loss: 1.5662, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [341/388], Loss: 1.5297, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [342/388], Loss: 1.4702, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [343/388], Loss: 1.6311, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [344/388], Loss: 1.6258, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [345/388], Loss: 1.9006, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [346/388], Loss: 1.9121, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [347/388], Loss: 1.7946, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [348/388], Loss: 1.8765, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [349/388], Loss: 1.5233, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [350/388], Loss: 1.4709, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [351/388], Loss: 1.5320, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [352/388], Loss: 1.9306, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [353/388], Loss: 1.5267, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [354/388], Loss: 1.6255, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [355/388], Loss: 1.6403, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [356/388], Loss: 1.7851, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [357/388], Loss: 1.3830, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [358/388], Loss: 1.5870, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [359/388], Loss: 1.0028, Accuracy: 68.75%\n",
      "Epoch [1/5], Batch [360/388], Loss: 1.5938, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [361/388], Loss: 1.5214, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [362/388], Loss: 2.3085, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [363/388], Loss: 1.6984, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [364/388], Loss: 1.3402, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [365/388], Loss: 1.0623, Accuracy: 65.62%\n",
      "Epoch [1/5], Batch [366/388], Loss: 1.6310, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [367/388], Loss: 1.4723, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [368/388], Loss: 1.6951, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [369/388], Loss: 1.5960, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [370/388], Loss: 1.4271, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [371/388], Loss: 1.9627, Accuracy: 34.38%\n",
      "Epoch [1/5], Batch [372/388], Loss: 1.2949, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [373/388], Loss: 1.9063, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [374/388], Loss: 1.8275, Accuracy: 50.00%\n",
      "Epoch [1/5], Batch [375/388], Loss: 1.7024, Accuracy: 46.88%\n",
      "Epoch [1/5], Batch [376/388], Loss: 1.2728, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [377/388], Loss: 1.1255, Accuracy: 68.75%\n",
      "Epoch [1/5], Batch [378/388], Loss: 1.7638, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [379/388], Loss: 1.3703, Accuracy: 65.62%\n",
      "Epoch [1/5], Batch [380/388], Loss: 1.5649, Accuracy: 59.38%\n",
      "Epoch [1/5], Batch [381/388], Loss: 1.6355, Accuracy: 56.25%\n",
      "Epoch [1/5], Batch [382/388], Loss: 1.3074, Accuracy: 53.12%\n",
      "Epoch [1/5], Batch [383/388], Loss: 1.8602, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [384/388], Loss: 1.7702, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [385/388], Loss: 1.5769, Accuracy: 37.50%\n",
      "Epoch [1/5], Batch [386/388], Loss: 1.5592, Accuracy: 43.75%\n",
      "Epoch [1/5], Batch [387/388], Loss: 1.6838, Accuracy: 40.62%\n",
      "Epoch [1/5], Batch [388/388], Loss: 1.7187, Accuracy: 46.43%\n",
      "Epoch [1/5] completed in 13788.38 seconds, Loss: 2.3165, Accuracy: 43.43%\n",
      "Epoch [2/5], Batch [1/388], Loss: 1.7865, Accuracy: 34.38%\n",
      "Epoch [2/5], Batch [2/388], Loss: 1.4419, Accuracy: 50.00%\n",
      "Epoch [2/5], Batch [3/388], Loss: 1.3516, Accuracy: 59.38%\n",
      "Epoch [2/5], Batch [4/388], Loss: 1.5536, Accuracy: 56.25%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy function\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    return torch.sum(preds == labels).item() / labels.size(0)\n",
    "\n",
    "# Model Training\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += accuracy(outputs, labels) * images.size(0)\n",
    "        total_samples += images.size(0)\n",
    "        print(f\"Epoch [{epoch+1}/5], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}, Accuracy: {accuracy(outputs, labels) * 100:.2f}%\")\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/5] completed in {epoch_duration:.2f} seconds, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc * 100:.2f}%\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Saving\n",
    "model_path = \"fusion_model_offline.pkl\"\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
